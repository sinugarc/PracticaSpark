{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sinugarc/PracticaSpark/blob/main/Destino_preferido.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFrwHAcEsJZM",
        "outputId": "f45a34d4-4dca-4a32-fa0b-3a6b282a61cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=cce32342cffaf144d0646735f7833e60fe5fe80fae2d0a2fbff756951d9968a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "import pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getTpla(x):\n",
        "  tpla = (x['id'],\n",
        "          x['name'],\n",
        "          float(x['latitude']),\n",
        "          float(x['longitude']))\n",
        "  return tpla\n",
        "\n",
        "def identifyZone(x,n,min_lat,cte_lat,min_long,cte_long):\n",
        "    zona_lat=(x[2]-min_lat)//cte_lat\n",
        "    zona_long=(x[3]-min_long)//cte_long\n",
        "    zona = zona_lat*n + zona_long\n",
        "    return (int(zona),x[0])\n",
        "\n",
        "def agruparZona(sc,filename,n):\n",
        "    data = sc.textFile(filename)\n",
        "    mov = data.map(lambda x: json.loads(x)).take(1)[0][\"stations\"] \n",
        "    \n",
        "    datos=sc.parallelize(mov)\n",
        "    estaciones = datos.map(getTpla)\n",
        "    \n",
        "    \n",
        "    max_long=max(estaciones,key=(lambda x: x[3]))[3]\n",
        "    min_long=min(estaciones,key=(lambda x: x[3]))[3]\n",
        "    max_lat=max(estaciones,key=(lambda x: x[2]))[2]\n",
        "    min_lat=min(estaciones,key=(lambda x: x[2]))[2]\n",
        "    \n",
        "    cte_long=(max_long-min_long)/float(n)\n",
        "    cte_lat=(max_lat-min_lat)/float(n)\n",
        "    \n",
        "    lista_zonas=estaciones.map(lambda x : identifyZone(x,n,min_lat,cte_lat,min_long,cte_long))\\\n",
        "                          .groupByKey().mapValues(lambda x : list(x))\n",
        "\n",
        "    return lista_zonas.collectAsMap()\n",
        "    \n",
        "def getTpla2(x):\n",
        "  tpla = (x['idunplug_station'],\n",
        "           x['idplug_station'],\n",
        "           x['travel_time'])\n",
        "  return tpla\n",
        "  \n",
        "def elegir_preferido(lista, perc, opcion):\n",
        "    total=0\n",
        "    for i in lista:\n",
        "      total+=i[0]\n",
        "    j=0\n",
        "    L=[]\n",
        "    if opcion==1:\n",
        "      while lista[j][0]>perc*total:\n",
        "          print(lista[j][1])\n",
        "          L.append((lista[j][1],lista[j][0]))\n",
        "          j+=1\n",
        "    else:\n",
        "      acum2=0\n",
        "      while acum2<perc*total:\n",
        "            acum2+=lista[j][0]\n",
        "            L.append((lista[j][1],lista[j][0]))\n",
        "            j+=1\n",
        "  \n",
        "    return total, L\n",
        "    \n",
        "\n",
        "def cambiar_de_id_zona(ida, dict_lista_zonas):\n",
        "    for key in dict_lista_zonas:\n",
        "      if ida[1] in dict_lista_zonas[key]:\n",
        "        return (key,1)\n",
        "\n",
        "def F(sc, zona_a_analizar, lista_zonas, infile1, outfile, perc, opcion):\n",
        "    rdd_base = sc.textFile(infile1)\n",
        "    bicis = rdd_base.map(lambda x: json.loads(x))\n",
        "    movimientos = bicis.map(getTpla2)\\\n",
        "                       .filter(lambda x: x[2] >= 700 and x[2]<=1000 ) \n",
        "    \n",
        "    id_zona= lista_zonas[zona_a_analizar]  \n",
        "    rdd= movimientos.filter(lambda x: x[0] in id_zona)\\\n",
        "                    .map(lambda x: cambiar_de_id_zona(x,lista_zonas))\\\n",
        "                    .groupByKey()\\\n",
        "                    .mapValues(lambda x: len(x))\\\n",
        "                    .map(lambda x: (x[1],x[0]))\\\n",
        "                    .sortByKey(False)\n",
        "    \n",
        "    total, L=elegir_preferido(rdd.collect(), perc ,opcion)\n",
        "\n",
        "    outf = open(outfile, \"w\")\n",
        "    outf.write(f'Los movimientos registrados, que salen de la zona {zona_a_analizar}, son {total}. \\nEstos provienen de las estaciones {lista_zonas[zona_a_analizar]} \\n\\n')\n",
        "    if opcion==1:\n",
        "      outf.write(f'Las zonas preferidas con porcentaje mayor que {perc*100}% son: \\n\\n')\n",
        "    else:\n",
        "      outf.write(f'Las zonas que en total suman un porcentaje de viajes mayor que {perc*100}% son: \\n')\n",
        "    for line in L:\n",
        "        p=int(line[1]/total*10000)/100.\n",
        "        outf.write(f'- Zona {line[0]} con {line[1]} viajes que acumula el {p}% de los viajes\\nLas estaciones pertenecientes a esta zona son {lista_zonas[line[0]]} \\n')\n",
        "    if len(L)==0:\n",
        "        outf.write('Ninguna zona cumple los requisitos solicitados')\n",
        "    \n",
        "    outf.close()\n",
        "\n",
        "\n",
        "def main(infile1,infile2,outfile,zonaSetUp,zona,perc,opcion):\n",
        "    sc=SparkContext()\n",
        "    sc.setLogLevel(\"ERROR\")\n",
        "    b= agruparZona(sc,infile2,zonaSetUp)\n",
        "    F(sc,zona, b, infile1,outfile,perc, opcion)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if len(sys.argv) != 8:\n",
        "        print(\"Uso: python3 {0} <fileInMovements> <fileInStations> <fileOut> <#zoneSetUp> <targetZone> < % > < Option >\".format(sys.argv[0]))\n",
        "    else:\n",
        "        p=float(sys.argv[6])\n",
        "        if p>1:\n",
        "            p=p/100.\n",
        "        main(sys.argv[1],sys.argv[2],sys.argv[3],int(sys.argv[4]),int(sys.argv[5]),p,int(sys.argv[7]))\n",
        "    "
      ],
      "metadata": {
        "id": "d8Ar0QfGD8c4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}