{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sinugarc/PracticaSpark/blob/main/Destino_preferido.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFrwHAcEsJZM",
        "outputId": "4dacbdda-7b01-4257-954b-7576417a6c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=97f1792a75f5acdeb16c51682f34f9bca787a8499f56e438231892451ca7635e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "import pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "from pyspark import SparkContext\n",
        "import functools\n",
        "import os\n",
        "\n",
        "def getTpla(x):\n",
        "  tpla = (x['id'],\n",
        "          x['name'],\n",
        "          float(x['latitude']),\n",
        "          float(x['longitude']))\n",
        "  return tpla\n",
        "\n",
        "def identifyZone(x,min_lat,cte_lat,min_long,cte_long):\n",
        "    zona_lat=(x[2]-min_lat)//cte_lat\n",
        "    zona_long=(x[3]-min_long)//cte_long\n",
        "    zona = zona_lat*5 + zona_long\n",
        "    return (int(zona),x[0])\n",
        "\n",
        "def agruparZona(sc,filename,n):\n",
        "    #with SparkContext() as sc:\n",
        "        sc.setLogLevel(\"ERROR\")\n",
        "        data = sc.textFile(filename)\n",
        "        mov = data.map(lambda x: json.loads(x)).take(1)[0][\"stations\"] #quitar take y arreglar resto\n",
        "            \n",
        "        estaciones = list(map(getTpla,mov))\n",
        "        est = map(getTpla,mov)\n",
        "        \n",
        "        max_long=max(estaciones,key=(lambda x: x[3]))[3]\n",
        "        min_long=min(estaciones,key=(lambda x: x[3]))[3]\n",
        "        max_lat=max(estaciones,key=(lambda x: x[2]))[2]\n",
        "        min_lat=min(estaciones,key=(lambda x: x[2]))[2]\n",
        "        \n",
        "        cte_long=(max_long-min_long)/float(n)\n",
        "        cte_lat=(max_lat-min_lat)/float(n)\n",
        "        \n",
        "        lista_zonas=list(map(lambda x : identifyZone(x,min_lat,cte_lat,min_long,cte_long),est))\n",
        "        a=sc.parallelize(lista_zonas)\n",
        "        a=a.groupByKey().mapValues(lambda x : list(x))\n",
        "\n",
        "           \n",
        "        print(a.collectAsMap())\n",
        "        return a.collectAsMap()\n",
        "    \n",
        "def getTpla2(x):\n",
        "  tpla = (x['idunplug_station'],\n",
        "           x['idplug_station'],\n",
        "           x['travel_time'])\n",
        "  return tpla\n",
        "  \n",
        "def elegir_preferido(lista, perc, opcion):\n",
        "    total=0\n",
        "    for i in lista:\n",
        "      total+=i[0]\n",
        "    j=0\n",
        "    L=[]\n",
        "    if opcion==1:\n",
        "      while lista[j][0]>perc*total:\n",
        "          print(lista[j][1])\n",
        "          L.append(lista[j][1])\n",
        "          j+=1\n",
        "    else:\n",
        "      acum2=0\n",
        "      while acum2<perc*total:\n",
        "            acum2+=lista[j][0]\n",
        "            L.append(lista[j][1])\n",
        "            j+=1\n",
        "  \n",
        "    return total, L\n",
        "    \n",
        "\n",
        "def cambiar_de_id_zona(ida, dict_lista_zonas):\n",
        "    for key in dict_lista_zonas:\n",
        "      if ida[1] in dict_lista_zonas[key]:\n",
        "        return (key,1)\n",
        "\n",
        "def F(sc, zona_a_analizar, lista_zonas, infile1, outfile, perc, opcion):\n",
        "    rdd_base = sc.textFile(infile1)\n",
        "    bicis = rdd_base.map(lambda x: json.loads(x))\n",
        "    movimientos = bicis.map(getTpla2)\\\n",
        "                  .filter(lambda x: x[2] >= 700 and x[2]<=1000 ) \n",
        "    print('1',movimientos)\n",
        "    id_zona= lista_zonas[zona_a_analizar]  \n",
        "    rdd= movimientos.filter(lambda x: x[0] in id_zona)\\\n",
        "                    .map(lambda x: cambiar_de_id_zona(x,lista_zonas))\\\n",
        "                    .groupByKey()\\\n",
        "                    .mapValues(lambda x: len(x))\\\n",
        "                    .map(lambda x: (x[1],x[0]))\\\n",
        "                    .sortByKey(False)\n",
        "    print(rdd.collect())\n",
        "    total, L=elegir_preferido(rdd.collect(), perc ,opcion)\n",
        "\n",
        "    outf = open(outfile, \"w\")\n",
        "    outf.write(f'El total de movimientos es {total} que salen de la zona {zona_a_analizar} \\n')\n",
        "    if opcion==1:\n",
        "      outf.write(f'Las zonas preferidas con porcentaje mayor que {perc} son: \\n')\n",
        "    else:\n",
        "      outf.write(f'Las zonas que suman al menos un porcentaje mayor que {perc} son: \\n')\n",
        "    for line in L:\n",
        "        outf.write(str(line) + '\\n')\n",
        "    \n",
        "    outf.close()\n",
        "\n",
        "\n",
        "def main(infile1,infile2,outfile,zona,perc, opcion):\n",
        "    #sc=SparkContext()\n",
        "    b= agruparZona(sc,infile2,5)\n",
        "    F(sc,zona, b, infile1,outfile,perc, opcion)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main('202007_movements.json', '202007.json', 'prueba.txt', 12, 0.10, 1)\n",
        "    #Pasar un diccionario con los datos\n",
        "    #Escribirlo con parametros \n",
        "    #arreglar lo del sc\n",
        "    #Readme\n",
        "    "
      ],
      "metadata": {
        "id": "-DqNVyHyGRNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe1e45b-36e5-4d40-a6f3-8f826d165cf6"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{6: [1, 2, 3, 10, 19, 25, 26, 27, 28, 29, 32, 35, 36, 37, 38, 39, 43, 44, 45, 46, 47, 48, 56, 57, 60, 218, 219], 12: [9, 11, 20, 30, 92, 93, 95, 97, 98, 99, 100, 108, 109, 110, 111, 112, 113, 130, 164, 165, 167, 198, 206], 10: [15, 61, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 131, 132, 207], 8: [65, 66, 67, 74, 75, 78, 79, 80, 87, 88, 189], 14: [106, 192, 193, 195, 196, 201], 2: [126, 127, 128, 136, 176, 180, 184, 185], 16: [161, 157, 156, 149, 150, 209, 210, 212], 18: [155, 148, 144, 202, 203, 204], 22: [143, 141, 140, 137, 138, 173, 216, 217], 0: [177, 220], 4: [187], 11: [4, 5, 6, 7, 8, 12, 13, 14, 16, 17, 18, 58, 59, 62, 63, 169, 163, 168, 208], 7: [21, 31, 33, 54, 55, 64, 69, 71, 72, 73, 84, 85, 86, 91, 107], 5: [34, 40, 41, 42, 49, 174, 175], 1: [50, 51, 52, 53, 129, 133, 134, 135, 178, 179, 181, 182, 183], 9: [76, 77, 190], 3: [81, 82, 83, 89, 186, 188], 13: [90, 94, 96, 101, 102, 103, 104, 105, 114, 115, 166, 170, 171, 191, 194, 199, 200], 15: [160, 197, 211], 17: [153, 158, 151, 152, 154, 159, 162, 146, 147, 145], 21: [142, 139, 213, 214, 215], 23: [172, 205]}\n",
            "1 PythonRDD[623] at RDD at PythonRDD.scala:53\n",
            "[(1795, 6), (1283, 1), (1054, 10), (1054, 12), (1035, 8), (1012, 11), (966, 22), (929, 13), (837, 5), (772, 2), (735, 16), (675, 3), (673, 21), (640, 18), (553, 14), (539, 17), (527, 7), (403, 15), (346, 4), (288, 23), (268, 0), (253, 9)]\n",
            "6\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}