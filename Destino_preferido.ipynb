{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sinugarc/PracticaSpark/blob/main/Destino_preferido.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "import pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFrwHAcEsJZM",
        "outputId": "5a5e0c4c-6872-49a6-cf84-1560f3602c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=e171f30b7a7349fad57fb5fffdc62c7be6e53a4fa0e7b92f52b6dd1667a6637b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "-QhngDXdrt7G",
        "outputId": "9ed09f8e-93cb-4f45-d7ae-22aaa52685ff"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2208d21eab4e>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mhadoop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhadoop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhadoop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileSystem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    196\u001b[0m             )\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    446\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-9-2208d21eab4e>:57 "
          ]
        }
      ],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "from pyspark import SparkContext\n",
        "import functools\n",
        "import os\n",
        "\n",
        "def agruparZona(sc, infile2):\n",
        "    rdd_base = sc.textFile(infile2)\n",
        "    estaciones = rdd_base.map(lambda x: json.loads(x)).take(1) \n",
        "    estaciones = estaciones['estaciones'] #[{..., long:40 lat:40  id:1}]\n",
        "    \n",
        "    \n",
        "def getTpla(x):\n",
        "  tpla = (x['idunplug_station'],\n",
        "           x['idplug_station'],\n",
        "           x['travel_time'])\n",
        "  return tpla\n",
        "  \n",
        "\n",
        "def cambiar_de_id_zona(id, dict_lista_zonas):\n",
        "    for key, value in dict_lista_zonas:\n",
        "      if id in value:\n",
        "        return key\n",
        "\n",
        "def F(sc, zona_a_analizar, lista_zonas, infile1, outfile):\n",
        "    rdd_base = sc.textFile(infile1)\n",
        "    bicis = rdd_base.map(lambda x: json.loads(x))\n",
        "    movimientos = bicis.map(getTpla)\\\n",
        "                  .filter() #segun el 3 argumento, acotar respecto a ciertos segundos)  [ (idi,idf,t)]\n",
        "    id_zona= lista_zonas[zona_a_analizar]\n",
        "    rdd= movimientos.filter(lambda x: idi in id_zona)\\  #[(id,idf,t)] que nos interesan\n",
        "                    .map(lambda x: cambiar_de_id_zona(x[1],lista_zonas)) #[zonaf, zonaf, zonaf]\n",
        "                    .groupByKey #(si existe) {zonaf: cont, ...}\n",
        "    #tomamos el maximo, el porcentaje ,...               \n",
        "    \n",
        "   \n",
        "    \n",
        "    outf = open(outfile, \"w\")\n",
        "    for line in to_print:\n",
        "        outf.write(line + '\\n')\n",
        "    outf.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sc = SparkContext()\n",
        "    hadoop = sc._jvm.org.apache.hadoop\n",
        "    fs = hadoop.fs.FileSystem\n",
        "    conf = hadoop.conf.Configuration()\n",
        "    path = hadoop.fs.Path('/public_data/bicimad')\n",
        "    cwd = os.getcwd()\n",
        "    filenames = [str(f.getPath()) for f in fs.get(conf).listStatus(path)]\n",
        "    date_files = [(f, pathInToOut(f, \"deficit\")) for f in filenames if 'stations' not in f]\n",
        "    \n",
        "    \n",
        "    if not os.path.isdir(\"data\"):\n",
        "        os.mkdir(\"data\")\n",
        "        print(\"Created /data directory in wd\")\n",
        "    for date_file in date_files:\n",
        "        ifile = date_file[0]\n",
        "        ofile = date_file[1][0]\n",
        "        year_dir = \"data/\" + date_file[1][0]\n",
        "        if not os.path.isdir(year_dir):\n",
        "            os.mkdir(year_dir)\n",
        "            print(\"Created \", year_dir, \" for that year's data archive\")\n",
        "        print(\"Working on \", ifile, \" to store in \", date_file[1][1])\n",
        "        rawToStd(sc, ifile, ofile)\n",
        "        print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x5EVBBpOr_TG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}